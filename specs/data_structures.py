class WorldGraph:
    """
    The complete experience graph - all memories and their relationships
    Storage and retrieval for the robot's world model
    """
    
    def __init__(self):
        self.nodes: Dict[str, ExperienceNode] = {}
        self.latest_node_id: Optional[str] = None
        self.total_nodes_created: int = 0
        self.total_merges_performed: int = 0
        
        # Performance optimization structures
        self.strength_index: Dict[float, Set[str]] = {}  # strength -> set of node_ids
        self.temporal_chain: List[str] = []              # chronological order
        self.similarity_clusters: Dict[str, Set[str]] = {}  # cluster_id -> node_ids
    
    def has_nodes(self) -> bool:
        """Check if graph has any nodes (for bootstrap detection)"""
        return len(self.nodes) > 0
    
    def has_nodes(self) -> bool:
        """Check if graph has any nodes (for bootstrap detection)"""
        return len(self.nodes) > 0
    
    def add_node(self, experience: ExperienceNode) -> str:
        """Add a new experience node to the graph"""
        node_id = experience.node_id
        self.nodes[node_id] = experience
        self.total_nodes_created += 1
        
        # Update indexes
        self._update_strength_index(node_id, experience.strength)
        self.temporal_chain.append(node_id)
        
        # Link to previous node temporally
        if self.latest_node_id:
            experience.temporal_predecessor = self.latest_node_id
            if self.latest_node_id in self.nodes:
                self.nodes[self.latest_node_id].temporal_successor = node_id
        
        self.latest_node_id = node_id
        return node_id"""
Core Data Structures for Emergent Intelligence Robot
Defines the fundamental data types for experience storage and world modeling
"""

from typing import List, Optional, Dict, Any, Set
from dataclasses import dataclass, field
from datetime import datetime
import uuid


@dataclass
class ExperienceNode:
    """
    A single moment of experience: prediction + action + reality
    The fundamental unit of learning and memory
    """
    # Core experience data
    mental_context: List[float]           # Current brain state when this experience occurred
    action_taken: Dict[str, float]        # Motor commands executed {actuator_id: value}
    predicted_sensory: List[float]        # What we expected to sense
    actual_sensory: List[float]           # What actually happened
    prediction_error: float               # Magnitude of prediction miss
    
    # Node metadata
    node_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    strength: float = 1.0                 # Usage-based reinforcement
    timestamp: datetime = field(default_factory=datetime.now)
    
    # Graph connections
    temporal_predecessor: Optional[str] = None     # Previous experience in time
    temporal_successor: Optional[str] = None       # Next experience in time
    prediction_sources: List[str] = field(default_factory=list)  # Nodes that predicted this
    similar_contexts: List[str] = field(default_factory=list)    # Similar experience nodes
    
    # Learning tracking
    times_accessed: int = 0               # How often this node was used in predictions
    last_accessed: datetime = field(default_factory=datetime.now)
    merge_count: int = 0                  # How many nodes have been merged into this one
    
    def __hash__(self) -> int:
        return hash(self.node_id)
    
    def __eq__(self, other) -> bool:
        if not isinstance(other, ExperienceNode):
            return False
        return self.node_id == other.node_id


@dataclass
class PredictionPacket:
    """
    A prediction generated by the brain, waiting to be validated against reality
    """
    expected_sensory: List[float]         # Predicted sensor values
    motor_action: Dict[str, float]        # Motor commands to execute
    confidence: float                     # How certain we are (0.0 to 1.0)
    timestamp: datetime                   # When prediction was made
    sequence_id: int                      # For synchronization with brainstem
    
    # Metadata about how prediction was generated
    traversal_paths: List[List[str]] = field(default_factory=list)  # Node IDs used
    consensus_strength: str = "unknown"   # 'strong', 'weak', 'random'
    thinking_depth: int = 0               # How many traversal steps


@dataclass
class SensoryPacket:
    """
    Current sensor readings from the brainstem
    """
    sensor_values: List[float]            # Raw sensor data [sensor_0, sensor_1, ...]
    actuator_positions: List[float]       # Current actuator states
    timestamp: datetime                   # When readings were taken
    sequence_id: int                      # For synchronization
    
    # Brainstem metadata
    brainstem_prediction: Optional[Dict[str, float]] = None  # Client-side predicted values
    network_latency: float = 0.0          # Measured round-trip time


class WorldGraph:
    """
    The complete experience graph - all memories and their relationships
    Storage and retrieval for the robot's world model
    """
    
    def __init__(self):
        self.nodes: Dict[str, ExperienceNode] = {}
        self.latest_node_id: Optional[str] = None
        self.total_nodes_created: int = 0
        self.total_merges_performed: int = 0
        
        # Performance optimization structures
        self.strength_index: Dict[float, Set[str]] = {}  # strength -> set of node_ids
        self.temporal_chain: List[str] = []              # chronological order
        self.similarity_clusters: Dict[str, Set[str]] = {}  # cluster_id -> node_ids
    
    def add_node(self, experience: ExperienceNode) -> str:
        """Add a new experience node to the graph"""
        node_id = experience.node_id
        self.nodes[node_id] = experience
        self.total_nodes_created += 1
        
        # Update indexes
        self._update_strength_index(node_id, experience.strength)
        self.temporal_chain.append(node_id)
        
        # Link to previous node temporally
        if self.latest_node_id:
            experience.temporal_predecessor = self.latest_node_id
            if self.latest_node_id in self.nodes:
                self.nodes[self.latest_node_id].temporal_successor = node_id
        
        self.latest_node_id = node_id
        return node_id
    
    def get_node(self, node_id: str) -> Optional[ExperienceNode]:
        """Retrieve a node by ID"""
        return self.nodes.get(node_id)
    
    def remove_node(self, node_id: str) -> bool:
        """Remove a node and clean up all references"""
        if node_id not in self.nodes:
            return False
        
        node = self.nodes[node_id]
        
        # Clean up all references to this node
        for other_id, other_node in self.nodes.items():
            if other_id == node_id:
                continue
                
            # Remove from similarity lists
            if node_id in other_node.similar_contexts:
                other_node.similar_contexts.remove(node_id)
            
            # Remove from prediction sources
            if node_id in other_node.prediction_sources:
                other_node.prediction_sources.remove(node_id)
            
            # Fix temporal chain
            if other_node.temporal_predecessor == node_id:
                other_node.temporal_predecessor = node.temporal_predecessor
            if other_node.temporal_successor == node_id:
                other_node.temporal_successor = node.temporal_successor
        
        # Remove from indexes
        self._remove_from_strength_index(node_id, node.strength)
        if node_id in self.temporal_chain:
            self.temporal_chain.remove(node_id)
        
        # Remove the node
        del self.nodes[node_id]
        return True
    
    def find_similar_nodes(self, target_context: List[float], 
                          similarity_threshold: float = 0.7,
                          max_results: int = 10) -> List[ExperienceNode]:
        """Find nodes with similar mental contexts"""
        similarities = []
        
        for node in self.nodes.values():
            similarity = self._calculate_context_similarity(target_context, node.mental_context)
            if similarity >= similarity_threshold:
                similarities.append((similarity, node))
        
        # Sort by similarity (highest first) and limit results
        similarities.sort(key=lambda x: x[0], reverse=True)
        return [node for _, node in similarities[:max_results]]
    
    def get_nodes_by_strength_range(self, min_strength: float, 
                                   max_strength: float = float('inf')) -> List[ExperienceNode]:
        """Get all nodes within a strength range"""
        result = []
        for node in self.nodes.values():
            if min_strength <= node.strength <= max_strength:
                result.append(node)
        return result
    
    def get_weakest_nodes(self, count: int) -> List[ExperienceNode]:
        """Get the N weakest nodes (candidates for merging)"""
        all_nodes = list(self.nodes.values())
        all_nodes.sort(key=lambda n: n.strength)
        return all_nodes[:count]
    
    def node_count(self) -> int:
        """Total number of nodes in graph"""
        return len(self.nodes)
    
    def node_exists(self, node_id: str) -> bool:
        """Check if a node exists"""
        return node_id in self.nodes
    
    def all_nodes(self) -> List[ExperienceNode]:
        """Get all nodes (use carefully - can be large)"""
        return list(self.nodes.values())
    
    def get_latest_node(self) -> Optional[ExperienceNode]:
        """Get the most recently added node"""
        if self.latest_node_id:
            return self.nodes.get(self.latest_node_id)
        return None
    
    def update_node_strength(self, node_id: str, new_strength: float):
        """Update a node's strength and maintain indexes"""
        if node_id not in self.nodes:
            return
        
        old_strength = self.nodes[node_id].strength
        self._remove_from_strength_index(node_id, old_strength)
        
        self.nodes[node_id].strength = new_strength
        self.nodes[node_id].times_accessed += 1
        self.nodes[node_id].last_accessed = datetime.now()
        
        self._update_strength_index(node_id, new_strength)
    
    def get_graph_statistics(self) -> Dict[str, Any]:
        """Get summary statistics about the graph"""
        if not self.nodes:
            return {"total_nodes": 0}
        
        strengths = [node.strength for node in self.nodes.values()]
        contexts_lengths = [len(node.mental_context) for node in self.nodes.values()]
        
        return {
            "total_nodes": len(self.nodes),
            "total_merges": self.total_merges_performed,
            "avg_strength": sum(strengths) / len(strengths),
            "max_strength": max(strengths),
            "min_strength": min(strengths),
            "avg_context_length": sum(contexts_lengths) / len(contexts_lengths),
            "temporal_chain_length": len(self.temporal_chain)
        }
    
    # Private helper methods
    
    def _calculate_context_similarity(self, context1: List[float], context2: List[float]) -> float:
        """Calculate similarity between two mental contexts using Euclidean distance"""
        if len(context1) != len(context2):
            return 0.0
        
        # Euclidean distance normalized to similarity score
        distance = sum((a - b) ** 2 for a, b in zip(context1, context2)) ** 0.5
        max_possible_distance = (len(context1) * 4.0) ** 0.5  # Assuming values roughly -2 to +2
        
        # Convert distance to similarity (0 to 1)
        similarity = max(0.0, 1.0 - (distance / max_possible_distance))
        return similarity
    
    def _update_strength_index(self, node_id: str, strength: float):
        """Update the strength-based index for fast retrieval"""
        # Round strength to bucket for indexing
        strength_bucket = round(strength, 2)
        if strength_bucket not in self.strength_index:
            self.strength_index[strength_bucket] = set()
        self.strength_index[strength_bucket].add(node_id)
    
    def _remove_from_strength_index(self, node_id: str, strength: float):
        """Remove node from strength index"""
        strength_bucket = round(strength, 2)
        if strength_bucket in self.strength_index:
            self.strength_index[strength_bucket].discard(node_id)
            if not self.strength_index[strength_bucket]:
                del self.strength_index[strength_bucket]


@dataclass
class MentalContext:
    """
    Represents the current state of the robot's mind
    What the robot is "thinking about" right now
    """
    context_vector: List[float]           # Compressed representation of current mental state
    recent_experiences: List[str]         # Node IDs of recent experiences
    active_predictions: List[str]         # Node IDs currently being predicted
    attention_focus: List[int]            # Indices into context_vector that have attention
    
    # Derived properties
    uncertainty_level: float = 0.0       # How uncertain the current predictions are
    novelty_level: float = 0.0           # How novel the current situation is
    
    def update_from_experience(self, new_experience: ExperienceNode, decay_rate: float = 0.1):
        """Update mental context based on new experience"""
        # This would implement the context evolution logic
        # For now, just track the experience
        self.recent_experiences.append(new_experience.node_id)
        
        # Keep only recent experiences
        if len(self.recent_experiences) > 10:
            self.recent_experiences = self.recent_experiences[-5:]


@dataclass
class GenomeData:
    """
    The robot's "DNA" - core drives and behavioral parameters
    Stored as data rather than code for easy experimentation
    """
    # Core drives (0.0 to 1.0)
    collective_survival_drive: float = 0.8
    curiosity_drive: float = 0.7
    competence_drive: float = 0.6
    social_bonding_drive: float = 0.5
    
    # Learning parameters
    base_learning_rate: float = 0.01
    memory_decay_rate: float = 0.001
    similarity_threshold: float = 0.7
    merge_threshold: float = 0.5
    
    # Behavioral tendencies
    exploration_bias: float = 0.3         # Tendency to try new things vs exploit known patterns
    deliberation_preference: float = 1.0   # How much thinking time to use
    social_sensitivity: float = 0.8       # How quickly to recognize other agents
    
    # Safety constraints
    max_motor_change_per_step: float = 0.1  # Prevent sudden dangerous movements
    emergency_stop_threshold: float = 10.0  # Stop if prediction error exceeds this
    
    def get_drive(self, drive_name: str, default: float = 0.5) -> float:
        """Get a drive value by name"""
        return getattr(self, f"{drive_name}_drive", default)


# Type aliases for clarity
NodeID = str
SensorReading = float
ActuatorCommand = float
Similarity = float