#!/usr/bin/env python3
"""
Hierarchical Experience Clustering

Implements emergent brain regionalization - experiences naturally cluster into
specialized regions, enabling fast hierarchical search for 10k+ experiences.

Biological inspiration: Brain regions specialize (visual cortex, motor cortex, etc.)
and hierarchical organization emerges naturally from experience patterns.
"""

import sys
import os
import time
import math
import numpy as np
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict

# Set up path to access brain modules
brain_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.join(brain_root, 'server', 'src'))
sys.path.append(os.path.join(brain_root, 'server'))

@dataclass
class ExperienceCluster:
    """A cluster of similar experiences (like a brain region)."""
    cluster_id: str
    center: List[float]  # Centroid of the cluster
    experiences: List[str]  # Experience IDs in this cluster
    specialization_score: float = 0.0  # How specialized this cluster is
    access_count: int = 0  # How often this cluster is accessed
    creation_time: float = 0.0
    
    def __post_init__(self):
        self.creation_time = time.time()
    
    def get_distance_to(self, vector: List[float]) -> float:
        """Compute distance from vector to cluster center."""
        if len(vector) != len(self.center):
            return float('inf')
        
        return np.linalg.norm(np.array(vector) - np.array(self.center))
    
    def add_experience(self, exp_id: str, vector: List[float]):
        """Add experience to cluster and update center."""
        self.experiences.append(exp_id)
        
        # Update center (running average)
        if len(self.experiences) == 1:
            self.center = vector.copy()
        else:
            # Exponential moving average for adaptive center
            alpha = 0.1  # Learning rate
            for i in range(len(self.center)):
                self.center[i] = (1 - alpha) * self.center[i] + alpha * vector[i]
        
        # Update specialization score based on cluster coherence
        self._update_specialization_score()
    
    def _update_specialization_score(self):
        """Update how specialized this cluster is."""
        if len(self.experiences) < 2:
            self.specialization_score = 0.0
            return
        
        # Specialization = inverse of cluster variance
        # More specialized clusters have experiences that are more similar
        # This encourages natural specialization
        self.specialization_score = 1.0 / (1.0 + len(self.experiences) * 0.1)

class HierarchicalExperienceIndex:
    """
    Hierarchical indexing system for fast experience retrieval.
    
    Creates emergent brain regions that specialize in different types of experiences.
    """
    
    def __init__(self, max_cluster_size: int = 50, similarity_threshold: float = 0.3):
        self.clusters: Dict[str, ExperienceCluster] = {}
        self.experience_to_cluster: Dict[str, str] = {}
        self.max_cluster_size = max_cluster_size
        self.similarity_threshold = similarity_threshold
        self.next_cluster_id = 0
        
        # Performance tracking
        self.total_searches = 0
        self.clusters_searched = 0
        self.experiences_searched = 0
        
    def add_experience(self, exp_id: str, vector: List[float]):
        """Add experience to the hierarchical index."""
        if exp_id in self.experience_to_cluster:
            return  # Already indexed
        
        # Find best cluster for this experience
        best_cluster = self._find_best_cluster(vector)
        
        if best_cluster is None:
            # Create new cluster (brain region specialization)
            cluster_id = f"cluster_{self.next_cluster_id}"
            self.next_cluster_id += 1
            
            new_cluster = ExperienceCluster(
                cluster_id=cluster_id,
                center=vector.copy(),
                experiences=[exp_id]
            )
            self.clusters[cluster_id] = new_cluster
            self.experience_to_cluster[exp_id] = cluster_id
            
            print(f"üß† New brain region created: {cluster_id} (specialization emerging)")
            
        else:
            # Add to existing cluster
            best_cluster.add_experience(exp_id, vector)
            self.experience_to_cluster[exp_id] = best_cluster.cluster_id
            
            # Check if cluster needs to split (like cortical column formation)
            if len(best_cluster.experiences) > self.max_cluster_size:
                self._split_cluster(best_cluster)
    
    def _find_best_cluster(self, vector: List[float]) -> Optional[ExperienceCluster]:
        """Find the best cluster for a vector."""
        best_cluster = None
        best_distance = float('inf')
        
        for cluster in self.clusters.values():
            distance = cluster.get_distance_to(vector)
            
            # Only consider clusters within similarity threshold
            if distance < self.similarity_threshold and distance < best_distance:
                best_distance = distance
                best_cluster = cluster
        
        return best_cluster
    
    def _split_cluster(self, cluster: ExperienceCluster):
        """Split a cluster into specialized sub-regions."""
        if len(cluster.experiences) <= self.max_cluster_size:
            return
        
        print(f"üß† Brain region {cluster.cluster_id} specializing (splitting into sub-regions)")
        
        # For now, simple split - in reality this would use more sophisticated clustering
        # This is where emergent specialization happens
        mid_point = len(cluster.experiences) // 2
        
        # Create two new specialized clusters
        cluster1_id = f"cluster_{self.next_cluster_id}"
        cluster2_id = f"cluster_{self.next_cluster_id + 1}"
        self.next_cluster_id += 2
        
        # Split experiences
        exp1 = cluster.experiences[:mid_point]
        exp2 = cluster.experiences[mid_point:]
        
        # Create new clusters with specialized centers
        cluster1 = ExperienceCluster(
            cluster_id=cluster1_id,
            center=cluster.center.copy(),
            experiences=exp1,
            specialization_score=cluster.specialization_score * 1.2  # More specialized
        )
        
        cluster2 = ExperienceCluster(
            cluster_id=cluster2_id,
            center=cluster.center.copy(),
            experiences=exp2,
            specialization_score=cluster.specialization_score * 1.2  # More specialized
        )
        
        # Update mappings
        for exp_id in exp1:
            self.experience_to_cluster[exp_id] = cluster1_id
        for exp_id in exp2:
            self.experience_to_cluster[exp_id] = cluster2_id
        
        # Replace old cluster with new specialized ones
        del self.clusters[cluster.cluster_id]
        self.clusters[cluster1_id] = cluster1
        self.clusters[cluster2_id] = cluster2
        
        print(f"üß† Specialization complete: {cluster1_id} and {cluster2_id}")
    
    def find_similar_experiences(self, query_vector: List[float], 
                               max_results: int = 10) -> List[Tuple[str, float]]:
        """
        Hierarchical search for similar experiences.
        
        First finds relevant clusters (brain regions), then searches within those.
        This is O(clusters + relevant_experiences) instead of O(all_experiences).
        """
        self.total_searches += 1
        
        # Step 1: Find relevant clusters (brain regions)
        relevant_clusters = []
        
        for cluster in self.clusters.values():
            distance = cluster.get_distance_to(query_vector)
            if distance < self.similarity_threshold * 2:  # Broader search at cluster level
                relevant_clusters.append((cluster, distance))
                cluster.access_count += 1  # Track region usage
        
        # Sort clusters by relevance
        relevant_clusters.sort(key=lambda x: x[1])
        
        # Step 2: Search within relevant clusters only
        candidate_experiences = []
        clusters_to_search = min(3, len(relevant_clusters))  # Limit cluster search
        
        self.clusters_searched += clusters_to_search
        
        for cluster, cluster_distance in relevant_clusters[:clusters_to_search]:
            # This is where we'd get actual experience vectors
            # For now, simulate with cluster distance as base similarity
            for exp_id in cluster.experiences:
                # Simulate experience similarity (in real implementation, 
                # this would use actual experience vectors)
                base_similarity = 1.0 / (1.0 + cluster_distance)
                noise = np.random.normal(0, 0.1)  # Add some variation
                similarity = max(0, min(1, base_similarity + noise))
                
                if similarity > 0.1:  # Only include meaningful similarities
                    candidate_experiences.append((exp_id, similarity))
        
        self.experiences_searched += len(candidate_experiences)
        
        # Step 3: Return top results
        candidate_experiences.sort(key=lambda x: x[1], reverse=True)
        return candidate_experiences[:max_results]
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get indexing statistics."""
        total_experiences = len(self.experience_to_cluster)
        
        cluster_sizes = [len(cluster.experiences) for cluster in self.clusters.values()]
        avg_cluster_size = sum(cluster_sizes) / len(cluster_sizes) if cluster_sizes else 0
        
        avg_experiences_per_search = self.experiences_searched / max(1, self.total_searches)
        avg_clusters_per_search = self.clusters_searched / max(1, self.total_searches)
        
        return {
            'total_experiences': total_experiences,
            'total_clusters': len(self.clusters),
            'avg_cluster_size': avg_cluster_size,
            'max_cluster_size': max(cluster_sizes) if cluster_sizes else 0,
            'avg_experiences_searched_per_query': avg_experiences_per_search,
            'avg_clusters_searched_per_query': avg_clusters_per_search,
            'search_efficiency': (total_experiences / max(1, avg_experiences_per_search)),
            'specialization_levels': [c.specialization_score for c in self.clusters.values()]
        }

def test_hierarchical_indexing():
    """Test hierarchical indexing performance."""
    print("üß† TESTING HIERARCHICAL BRAIN REGIONALIZATION")
    print("=" * 60)
    
    # Create index
    index = HierarchicalExperienceIndex(max_cluster_size=50, similarity_threshold=0.3)
    
    # Test different scales
    test_sizes = [100, 500, 1000, 5000, 10000]
    
    for size in test_sizes:
        print(f"\nüìä Testing {size} experiences...")
        
        # Add experiences with different patterns (like different brain functions)
        start_time = time.time()
        
        for i in range(size):
            # Create different types of experiences that should cluster
            if i % 3 == 0:  # "Visual" experiences
                base = [0.8, 0.2, 0.1, 0.1]
            elif i % 3 == 1:  # "Motor" experiences  
                base = [0.1, 0.8, 0.2, 0.1]
            else:  # "Memory" experiences
                base = [0.1, 0.1, 0.8, 0.2]
            
            # Add some variation
            noise = np.random.normal(0, 0.1, 4)
            vector = [max(0, min(1, base[j] + noise[j])) for j in range(4)]
            
            index.add_experience(f"exp_{i}", vector)
        
        setup_time = time.time() - start_time
        
        # Test search performance
        search_times = []
        for _ in range(10):
            query_vector = [0.5, 0.5, 0.5, 0.5]
            
            search_start = time.time()
            results = index.find_similar_experiences(query_vector, max_results=10)
            search_time = (time.time() - search_start) * 1000
            search_times.append(search_time)
        
        avg_search_time = sum(search_times) / len(search_times)
        stats = index.get_statistics()
        
        print(f"   Setup time: {setup_time:.2f}s")
        print(f"   Search time: {avg_search_time:.2f}ms")
        print(f"   Brain regions: {stats['total_clusters']}")
        print(f"   Avg region size: {stats['avg_cluster_size']:.1f}")
        print(f"   Search efficiency: {stats['search_efficiency']:.1f}x")
        print(f"   Experiences searched per query: {stats['avg_experiences_searched_per_query']:.1f}")
        
        # Check if we achieved the target
        if size >= 10000 and avg_search_time < 100:
            print(f"   üéâ SUCCESS: 10k+ experiences in {avg_search_time:.1f}ms!")
        elif size >= 10000:
            print(f"   ‚ö†Ô∏è  Close: 10k+ experiences in {avg_search_time:.1f}ms (target: <100ms)")
        
        # Stop if search gets too slow
        if avg_search_time > 500:
            print(f"   ‚ùå STOPPING: Search too slow")
            break
    
    # Show final specialization
    print(f"\nüß† FINAL BRAIN REGIONALIZATION:")
    final_stats = index.get_statistics()
    print(f"   Total brain regions: {final_stats['total_clusters']}")
    print(f"   Average region size: {final_stats['avg_cluster_size']:.1f}")
    print(f"   Search efficiency: {final_stats['search_efficiency']:.1f}x faster than linear")
    
    # Show most active regions
    active_clusters = sorted(index.clusters.values(), 
                           key=lambda c: c.access_count, 
                           reverse=True)[:5]
    
    print(f"\nüèÜ MOST ACTIVE BRAIN REGIONS:")
    for i, cluster in enumerate(active_clusters, 1):
        print(f"   {i}. {cluster.cluster_id}: {cluster.access_count} accesses, "
              f"{len(cluster.experiences)} experiences, "
              f"specialization: {cluster.specialization_score:.2f}")

if __name__ == "__main__":
    test_hierarchical_indexing()