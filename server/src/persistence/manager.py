"""
Persistence Manager

Main interface for brain state persistence. Handles saving and loading
of complete brain state using checkpoints and incremental deltas.
"""

import os
import json
import time
import uuid
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime

from .models import BrainCheckpoint, PersistenceConfig, SessionInfo
from .serializer import BrainStateSerializer


class PersistenceManager:
    """
    Brain Memory Persistence Manager
    
    Manages saving and loading of brain state to enable memory retention
    between sessions. Uses a hybrid checkpoint + incremental approach:
    
    - Checkpoints: Full brain state snapshots for reliable restoration
    - Deltas: Incremental changes for efficiency (planned for Phase 2)
    - Metadata: Session tracking and integrity validation
    
    Design Goals:
    1. Perfect state restoration - brain continues exactly where it left off
    2. Human-readable format - enable third-party analysis tools
    3. Efficient storage - avoid redundant data and excessive file sizes
    4. Debugging support - ability to inspect and replay brain evolution
    """
    
    def __init__(self, config: Optional[PersistenceConfig] = None):
        """
        Initialize persistence manager.
        
        Args:
            config: Persistence configuration, uses defaults if None
        """
        self.config = config or PersistenceConfig()
        self.serializer = BrainStateSerializer(use_compression=self.config.use_compression)
        print(f"🔧 Serializer compression: {self.config.use_compression}")
        
        # Current session tracking
        self.session_id = str(uuid.uuid4())[:8]
        self.session_start_time = time.time()
        self.checkpoints_created = []
        self.last_checkpoint_experience_count = 0
        self.last_checkpoint_time = time.time()
        self.checkpoint_counter = 0
        
        # Setup storage directories
        self._setup_directories()
        
        print(f"🗄️  PersistenceManager initialized")
        print(f"   Memory path: {self.memory_root}")
        print(f"   Session ID: {self.session_id}")
        print(f"   Checkpoint interval: {self.config.checkpoint_interval_experiences} experiences")
    
    @property
    def memory_root(self) -> Path:
        """Root directory for all persistent memory."""
        return Path(self.config.memory_root_path)
    
    @property  
    def checkpoints_dir(self) -> Path:
        """Directory for checkpoint files."""
        return self.memory_root / self.config.checkpoints_path
    
    @property
    def metadata_dir(self) -> Path:
        """Directory for metadata files."""
        return self.memory_root / self.config.metadata_path
    
    def _setup_directories(self) -> None:
        """Create necessary storage directories."""
        directories = [
            self.memory_root,
            self.checkpoints_dir,
            self.memory_root / self.config.deltas_path,
            self.metadata_dir
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
        
        # Create info file for external tools
        info_file = self.memory_root / "README.md"
        if not info_file.exists():
            self._create_info_file(info_file)
    
    def _create_info_file(self, info_file: Path) -> None:
        """Create documentation file for the memory directory."""
        content = """# Robot Memory Directory

This directory contains the persistent memory for the robot brain system.

## Structure

- `checkpoints/`: Complete brain state snapshots
- `deltas/`: Incremental changes between checkpoints (future)
- `metadata/`: Session logs and system information

## Checkpoint Format

Checkpoints are JSON files containing:
- **experiences**: All robot experiences with sensory data, actions, and outcomes
- **similarity_state**: Learned similarity parameters and weights  
- **activation_state**: Activation dynamics configuration
- **prediction_state**: Prediction engine statistics and patterns
- **stream_state**: Recent sensory-action stream buffer

## Analysis

The JSON format enables easy analysis with external tools:

```python
import json
with open('checkpoints/checkpoint_001000.json', 'r') as f:
    brain_state = json.load(f)
    
experiences = brain_state['brain_data']['experiences']
print(f"Brain has {len(experiences)} experiences")
```

## Session Tracking

- `session_log.json`: History of all brain sessions
- Each session records start/end times, experience counts, and checkpoints created

Generated by Robot Brain Persistence System
"""
        with open(info_file, 'w') as f:
            f.write(content)
    
    def should_create_checkpoint(self, current_experience_count: int) -> bool:
        """
        Check if it's time to create a checkpoint.
        
        Args:
            current_experience_count: Current number of experiences
            
        Returns:
            True if checkpoint should be created
        """
        # Experience-based trigger
        experience_diff = current_experience_count - self.last_checkpoint_experience_count
        if experience_diff >= self.config.checkpoint_interval_experiences:
            return True
        
        # Time-based trigger
        time_diff = time.time() - self.last_checkpoint_time
        if time_diff >= self.config.checkpoint_interval_seconds:
            return True
        
        return False
    
    def save_checkpoint(self, brain) -> str:
        """
        Save complete brain state checkpoint.
        
        Args:
            brain: MinimalBrain instance to save
            
        Returns:
            Checkpoint ID that was created
        """
        try:
            # Generate checkpoint info
            experience_count = len(brain.experience_storage._experiences)
            self.checkpoint_counter += 1
            checkpoint_id = f"checkpoint_{self.checkpoint_counter:06d}"
            timestamp = time.time()
            
            print(f"💾 Creating checkpoint {checkpoint_id} ({experience_count} experiences)...")
            
            # Extract brain state using serializer
            experiences_data = self.serializer.serialize_experiences(brain.experience_storage._experiences)
            similarity_data = self.serializer.serialize_similarity_state(brain.similarity_engine)
            activation_data = self.serializer.serialize_activation_state(brain.activation_dynamics)
            prediction_data = self.serializer.serialize_prediction_state(brain.prediction_engine)
            stream_data = self.serializer.serialize_stream_state(brain.stream_storage)
            
            # Gather performance stats
            performance_stats = brain.get_brain_stats()
            
            # Create learning metrics
            learning_metrics = self._extract_learning_metrics(brain)
            
            # Get system configuration
            system_config = self._get_system_config(brain)
            
            # Create checkpoint object
            checkpoint = BrainCheckpoint(
                checkpoint_id=checkpoint_id,
                timestamp=timestamp,
                experience_count=experience_count,
                session_id=self.session_id,
                brain_version="1.0",  # TODO: Get from brain system
                experiences=experiences_data,
                similarity_state=similarity_data,
                activation_state=activation_data,
                prediction_state=prediction_data,
                stream_state=stream_data,
                performance_stats=performance_stats,
                learning_metrics=learning_metrics,
                system_config=system_config
            )
            
            # Save to file
            checkpoint_file = self.checkpoints_dir / f"{checkpoint_id}.json"
            self.serializer.save_checkpoint(checkpoint, checkpoint_file)
            
            # Update tracking
            self.last_checkpoint_experience_count = experience_count
            self.last_checkpoint_time = timestamp
            self.checkpoints_created.append(checkpoint_id)
            
            # Update latest checkpoint pointer
            self._update_latest_checkpoint(checkpoint_id)
            
            # Log session activity
            self._log_checkpoint_created(checkpoint_id, experience_count)
            
            # Cleanup old checkpoints if needed
            self._cleanup_old_checkpoints()
            
            file_size = checkpoint_file.stat().st_size if checkpoint_file.exists() else 0
            if checkpoint_file.with_suffix('.json.gz').exists():
                file_size = checkpoint_file.with_suffix('.json.gz').stat().st_size
            
            print(f"✅ Checkpoint saved: {checkpoint_id} ({file_size/1024:.1f}KB)")
            
            return checkpoint_id
            
        except Exception as e:
            print(f"❌ Failed to save checkpoint: {e}")
            raise
    
    def load_latest_checkpoint(self) -> Optional[BrainCheckpoint]:
        """
        Load the most recent checkpoint.
        
        Returns:
            Latest BrainCheckpoint or None if no checkpoints exist
        """
        latest_file = self.metadata_dir / "latest_checkpoint.json"
        
        if not latest_file.exists():
            print("📂 No previous brain state found - starting fresh")
            return None
        
        try:
            # Read latest checkpoint info
            with open(latest_file, 'r') as f:
                latest_info = json.load(f)
            
            checkpoint_id = latest_info['checkpoint_id']
            checkpoint_file = self.checkpoints_dir / f"{checkpoint_id}.json"
            
            if not checkpoint_file.exists():
                # Try compressed version
                checkpoint_file = self.checkpoints_dir / f"{checkpoint_id}.json.gz"
                if not checkpoint_file.exists():
                    print(f"⚠️  Checkpoint file missing: {checkpoint_id}")
                    return None
            
            print(f"📂 Loading brain state from {checkpoint_id}...")
            
            # Load checkpoint
            checkpoint = self.serializer.load_checkpoint(checkpoint_file)
            
            print(f"✅ Brain state loaded: {checkpoint.experience_count} experiences")
            print(f"   Created: {datetime.fromtimestamp(checkpoint.timestamp).strftime('%Y-%m-%d %H:%M:%S')}")
            
            return checkpoint
            
        except Exception as e:
            print(f"❌ Failed to load checkpoint: {e}")
            return None
    
    def restore_brain_state(self, brain, checkpoint: BrainCheckpoint) -> None:
        """
        Restore brain to the state in the checkpoint.
        
        Args:
            brain: MinimalBrain instance to restore
            checkpoint: BrainCheckpoint to restore from
        """
        try:
            print(f"🔄 Restoring brain state from checkpoint {checkpoint.checkpoint_id}...")
            
            # Restore experiences
            experiences = self.serializer.deserialize_experiences(checkpoint.experiences)
            brain.experience_storage._experiences = experiences
            
            # Rebuild chronological order
            brain.experience_storage._chronological_order = sorted(
                experiences.keys(), 
                key=lambda exp_id: experiences[exp_id].timestamp
            )
            
            # Restore similarity engine state
            self.serializer.deserialize_similarity_state(checkpoint.similarity_state, brain.similarity_engine)
            
            # Restore activation engine state  
            self.serializer.deserialize_activation_state(checkpoint.activation_state, brain.activation_dynamics)
            
            # Restore prediction engine state
            self.serializer.deserialize_prediction_state(checkpoint.prediction_state, brain.prediction_engine)
            
            # Restore stream storage state
            self.serializer.deserialize_stream_state(checkpoint.stream_state, brain.stream_storage)
            
            # Update session tracking
            self.last_checkpoint_experience_count = checkpoint.experience_count
            
            # Restore brain counters
            brain.total_experiences = checkpoint.experience_count
            brain.total_predictions = checkpoint.performance_stats.get('brain_summary', {}).get('total_predictions', 0)
            
            print(f"✅ Brain state restored successfully")
            print(f"   Experiences: {len(experiences)}")
            print(f"   Similarity parameters: {len(checkpoint.similarity_state.get('feature_weights', []))}")
            
        except Exception as e:
            print(f"❌ Failed to restore brain state: {e}")
            raise
    
    def _extract_learning_metrics(self, brain) -> Dict[str, Any]:
        """Extract learning progress metrics from brain."""
        experiences = brain.experience_storage._experiences
        
        if not experiences:
            return {}
        
        # Calculate basic metrics
        total_experiences = len(experiences)
        active_experiences = sum(1 for exp in experiences.values() if exp.activation_level > 0)
        
        # Prediction error statistics
        errors = [exp.prediction_error for exp in experiences.values() if exp.prediction_error is not None]
        avg_error = sum(errors) / len(errors) if errors else 0
        
        # Recent performance (last 100 experiences)
        recent_experiences = sorted(experiences.values(), key=lambda x: x.timestamp)[-100:]
        recent_errors = [exp.prediction_error for exp in recent_experiences if exp.prediction_error is not None]
        recent_avg_error = sum(recent_errors) / len(recent_errors) if recent_errors else 0
        
        return {
            'total_experiences': total_experiences,
            'active_experiences': active_experiences,
            'activation_ratio': active_experiences / total_experiences if total_experiences > 0 else 0,
            'average_prediction_error': avg_error,
            'recent_prediction_error': recent_avg_error,
            'learning_improvement': max(0, avg_error - recent_avg_error) if avg_error > 0 else 0,
            'similarity_connections': sum(len(exp.similar_experiences) for exp in experiences.values()),
            'timestamp': time.time()
        }
    
    def _get_system_config(self, brain) -> Dict[str, Any]:
        """Get system configuration snapshot."""
        return {
            'similarity_engine': {
                'class': brain.similarity_engine.__class__.__name__,
                'learning_rate': getattr(brain.similarity_engine, 'learning_rate', 0.01)
            },
            'activation_engine': {
                'class': brain.activation_dynamics.__class__.__name__,
                'base_decay_rate': getattr(brain.activation_dynamics, 'base_decay_rate', 0.01)
            },
            'prediction_engine': {
                'class': brain.prediction_engine.__class__.__name__,
                'optimal_error_threshold': getattr(brain.prediction_engine, 'optimal_error_threshold', 0.1)
            },
            'persistence_config': self.config.to_dict()
        }
    
    def _update_latest_checkpoint(self, checkpoint_id: str) -> None:
        """Update pointer to latest checkpoint."""
        latest_info = {
            'checkpoint_id': checkpoint_id,
            'updated_at': time.time(),
            'session_id': self.session_id
        }
        
        latest_file = self.metadata_dir / "latest_checkpoint.json"
        with open(latest_file, 'w') as f:
            json.dump(latest_info, f, indent=2)
    
    def _log_checkpoint_created(self, checkpoint_id: str, experience_count: int) -> None:
        """Log checkpoint creation to session log."""
        session_log_file = self.metadata_dir / "session_log.json"
        
        # Load existing log
        sessions = []
        if session_log_file.exists():
            try:
                with open(session_log_file, 'r') as f:
                    sessions = json.load(f)
            except:
                sessions = []
        
        # Find or create current session entry
        current_session = None
        for session in sessions:
            if session['session_id'] == self.session_id:
                current_session = session
                break
        
        if not current_session:
            current_session = {
                'session_id': self.session_id,
                'start_time': self.session_start_time,
                'start_datetime': datetime.fromtimestamp(self.session_start_time).isoformat(),
                'checkpoints_created': [],
                'initial_experience_count': self.last_checkpoint_experience_count,
                'brain_version': '1.0'
            }
            sessions.append(current_session)
        
        # Add checkpoint to session
        current_session['checkpoints_created'].append({
            'checkpoint_id': checkpoint_id,
            'created_at': time.time(),
            'experience_count': experience_count
        })
        current_session['final_experience_count'] = experience_count
        
        # Save updated log
        with open(session_log_file, 'w') as f:
            json.dump(sessions, f, indent=2, default=str)
    
    def _cleanup_old_checkpoints(self) -> None:
        """Remove old checkpoints if we exceed the maximum."""
        if self.config.max_checkpoints <= 0:
            return
        
        # Get all checkpoint files
        checkpoint_files = list(self.checkpoints_dir.glob("checkpoint_*.json*"))
        
        if len(checkpoint_files) <= self.config.max_checkpoints:
            return
        
        # Sort by creation time and remove oldest
        checkpoint_files.sort(key=lambda f: f.stat().st_mtime)
        
        files_to_remove = checkpoint_files[:-self.config.max_checkpoints]
        for file_path in files_to_remove:
            try:
                file_path.unlink()
                print(f"🗑️  Cleaned up old checkpoint: {file_path.name}")
            except Exception as e:
                print(f"⚠️  Failed to remove {file_path.name}: {e}")
    
    def get_checkpoint_info(self) -> List[Dict[str, Any]]:
        """Get information about all available checkpoints."""
        checkpoints = []
        
        for checkpoint_file in sorted(self.checkpoints_dir.glob("checkpoint_*.json*")):
            try:
                # Extract checkpoint ID from filename
                checkpoint_id = checkpoint_file.stem.replace('.json', '')
                
                # Get file stats
                file_stats = checkpoint_file.stat()
                
                checkpoints.append({
                    'checkpoint_id': checkpoint_id,
                    'file_path': str(checkpoint_file),
                    'size_kb': file_stats.st_size / 1024,
                    'created_at': file_stats.st_mtime,
                    'created_datetime': datetime.fromtimestamp(file_stats.st_mtime).isoformat()
                })
                
            except Exception as e:
                print(f"⚠️  Error reading checkpoint {checkpoint_file}: {e}")
        
        return checkpoints
    
    def finalize_session(self) -> None:
        """Finalize the current session (call on brain shutdown)."""
        # Update session log with end time
        session_log_file = self.metadata_dir / "session_log.json"
        
        if session_log_file.exists():
            try:
                with open(session_log_file, 'r') as f:
                    sessions = json.load(f)
                
                # Find current session and update end time
                for session in sessions:
                    if session['session_id'] == self.session_id:
                        session['end_time'] = time.time()
                        session['end_datetime'] = datetime.now().isoformat()
                        break
                
                # Save updated log
                with open(session_log_file, 'w') as f:
                    json.dump(sessions, f, indent=2, default=str)
                    
            except Exception as e:
                print(f"⚠️  Failed to finalize session log: {e}")
        
        print(f"📋 Session {self.session_id} finalized ({len(self.checkpoints_created)} checkpoints created)")