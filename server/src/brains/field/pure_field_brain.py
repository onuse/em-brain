"""
PureFieldBrain: The Ultimate Synthesis
======================================
GPU-optimal + Biologically-inspired + Aggressively Simple + Hierarchically Scalable

All cognition through unified hierarchical field dynamics.
Sophistication emerges from scale and cross-resolution information flow.
True self-modification through meta-learning channels.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict, Any, Tuple, List
import math
from dataclasses import dataclass


@dataclass
class ScaleConfig:
    """Configuration for hierarchical scaling"""
    name: str
    levels: List[Tuple[int, int]]  # [(field_size, channels), ...]
    meta_channels: int  # Channels dedicated to meta-learning
    cross_scale_ratio: float  # Information flow between scales
    emergence_threshold: int  # Parameter count for emergent properties
    
    @property
    def total_params(self) -> int:
        """Calculate total parameter count"""
        params = 0
        for size, channels in self.levels:
            # Field parameters
            params += size ** 3 * channels
            # Evolution kernels (3x3x3 conv per level)
            params += channels * channels * 27
            # Cross-scale connections
            if len(self.levels) > 1:
                params += channels * channels  # Simplified cross-scale
        return params


# Predefined scaling configurations
SCALE_CONFIGS = {
    'hardware_constrained': ScaleConfig(
        name='hardware_constrained',
        levels=[(6, 64)],  # 6Â³Ã—64 for hardware constraints
        meta_channels=16,
        cross_scale_ratio=0.0,
        emergence_threshold=50_000
    ),
    'tiny': ScaleConfig(
        name='tiny',
        levels=[(16, 32)],
        meta_channels=8,
        cross_scale_ratio=0.0,
        emergence_threshold=100_000
    ),
    'small': ScaleConfig(
        name='small',
        levels=[(24, 48)],
        meta_channels=12,
        cross_scale_ratio=0.0,
        emergence_threshold=500_000
    ),
    'medium': ScaleConfig(
        name='medium',
        levels=[(32, 64)],
        meta_channels=16,
        cross_scale_ratio=0.0,
        emergence_threshold=2_000_000
    ),
    'large': ScaleConfig(
        name='large',
        levels=[(32, 96), (16, 48)],  # Two levels
        meta_channels=24,
        cross_scale_ratio=0.2,
        emergence_threshold=10_000_000
    ),
    'huge': ScaleConfig(
        name='huge',
        levels=[(48, 128), (24, 64), (12, 32)],  # Three levels
        meta_channels=32,
        cross_scale_ratio=0.3,
        emergence_threshold=50_000_000
    ),
    'massive': ScaleConfig(
        name='massive',
        levels=[(64, 256), (32, 128), (16, 64), (8, 32)],  # Four levels
        meta_channels=64,
        cross_scale_ratio=0.4,
        emergence_threshold=100_000_000
    )
}


class HierarchicalField(nn.Module):
    """A single level in the hierarchical field structure"""
    
    def __init__(
        self,
        field_size: int,
        channels: int,
        meta_channels: int,
        device: str = 'cuda'
    ):
        super().__init__()
        self.field_size = field_size
        self.channels = channels
        self.meta_channels = meta_channels
        self.device = device
        
        # The field at this resolution level
        self.field = torch.zeros(
            field_size, field_size, field_size, channels,
            device=device, dtype=torch.float32
        )
        
        # Evolution kernel for this level (learns its own dynamics)
        self.evolution_kernel = nn.Parameter(
            torch.randn(channels, channels, 3, 3, 3, device=device) * 0.02
        )
        
        # Meta-learning kernel (modifies evolution kernel)
        if meta_channels > 0:
            self.meta_kernel = nn.Parameter(
                torch.randn(meta_channels, meta_channels, 3, 3, 3, device=device) * 0.01
            )
        else:
            self.meta_kernel = None
            
    def evolve(
        self,
        diffusion_rate: float,
        decay_rate: float,
        noise_scale: float,
        meta_modulation: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """Evolve this field level"""
        # Reshape for 3D convolution
        field_reshaped = self.field.permute(3, 0, 1, 2).unsqueeze(0)
        
        # Apply evolution kernel (potentially modulated by meta-learning)
        kernel = self.evolution_kernel
        if meta_modulation is not None and self.meta_kernel is not None:
            # Meta-learning modulates the evolution kernel
            kernel = kernel * (1.0 + meta_modulation.mean() * 0.1)
        
        evolved = F.conv3d(
            field_reshaped,
            kernel,
            padding=1,
            groups=1
        )
        
        # Apply diffusion
        if diffusion_rate > 0:
            blur_kernel = torch.ones(1, 1, 3, 3, 3, device=self.device) / 27.0
            for c in range(self.channels):
                diffused = F.conv3d(
                    evolved[:, c:c+1],
                    blur_kernel,
                    padding=1
                )
                evolved[:, c:c+1] = (1 - diffusion_rate) * evolved[:, c:c+1] + \
                                   diffusion_rate * diffused
        
        # Apply decay
        evolved = evolved * decay_rate
        
        # Add noise
        if noise_scale > 0:
            # Noise is modulated by meta-channels
            if self.meta_channels > 0:
                meta_state = self.field[:, :, :, -self.meta_channels:].mean()
                noise_scale = noise_scale * (1.0 + meta_state * 0.5)
            noise = torch.randn_like(evolved) * noise_scale
            evolved = evolved + noise
        
        # Reshape back
        return evolved.squeeze(0).permute(1, 2, 3, 0)
    
    def update_meta_learning(self, error_signal: float):
        """Update meta-learning channels based on prediction error"""
        if self.meta_channels > 0 and self.meta_kernel is not None:
            # Meta-channels learn to modulate evolution
            meta_update = torch.randn_like(self.meta_kernel) * error_signal * 0.01
            self.meta_kernel.data += meta_update
            
            # Update meta-channels in field
            self.field[:, :, :, -self.meta_channels:] += error_signal * 0.05


class PureFieldBrain(nn.Module):
    """
    Hierarchically scalable field brain with true emergence at scale.
    
    Key innovations:
    - Multiple resolution levels for hierarchical processing
    - Cross-scale information flow for emergence
    - Meta-learning channels for self-modification
    - Massive parallelism through grouped convolutions
    - Scale-dependent emergent properties
    """
    
    def __init__(
        self,
        input_dim: int = 10,
        output_dim: int = 4,
        scale_config: Optional[ScaleConfig] = None,
        device: str = None,
        aggressive: bool = True
    ):
        super().__init__()
        
        # Device selection
        if device is None:
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.device = device
        
        # Use provided config or default to medium
        if scale_config is None:
            scale_config = SCALE_CONFIGS['medium']
        self.scale_config = scale_config
        
        # Dimensions
        self.input_dim = input_dim
        self.output_dim = output_dim
        
        # Create hierarchical field levels
        self.levels = nn.ModuleList()
        for field_size, channels in scale_config.levels:
            level = HierarchicalField(
                field_size=field_size,
                channels=channels,
                meta_channels=scale_config.meta_channels,
                device=device
            )
            self.levels.append(level)
        
        # Cross-scale connections (for multi-level configs)
        self.cross_scale_connections = nn.ModuleList()
        if len(self.levels) > 1:
            for i in range(len(self.levels) - 1):
                # Connection from level i to level i+1
                channels_from = scale_config.levels[i][1]
                channels_to = scale_config.levels[i+1][1]
                connection = nn.Parameter(
                    torch.randn(channels_to, channels_from, device=device) * 0.1
                )
                self.cross_scale_connections.append(connection)
        
        # Input/output projections (adapt to first/last level)
        first_channels = scale_config.levels[0][1]
        last_channels = scale_config.levels[-1][1] if len(scale_config.levels) > 1 else first_channels
        
        # Input projection: maps from variable sensory input to first level channels
        # Handle dynamic input dimension by using a linear layer instead of fixed matrix
        self.input_projection = nn.Linear(input_dim, first_channels, device=device, bias=False)
        
        # Output projection: maps from processing channels to motor outputs
        self.output_projection = nn.Parameter(
            torch.randn(last_channels, output_dim, device=device) * 0.1
        )
        
        # Aggressive parameters (scale-aware)
        if aggressive:
            # Scale learning rate with emergence threshold
            # Fix: Ensure learning rate is always positive
            scale_factor = max(0.1, min(2.0, math.log10(max(1, scale_config.total_params) / 1_000_000)))
            self.learning_rate = max(0.01, 0.2 * scale_factor)  # Always at least 0.01
            self.decay_rate = 0.98
            self.diffusion_rate = 0.1
            self.noise_scale = 0.05
            self.prediction_weight = 0.3
            
            # Emergent properties at scale
            if scale_config.total_params > scale_config.emergence_threshold:
                self.emergent_dynamics = True
                self.meta_learning_rate = 0.01
                self.cross_scale_strength = scale_config.cross_scale_ratio
            else:
                self.emergent_dynamics = False
                self.meta_learning_rate = 0.0
                self.cross_scale_strength = 0.0
        else:
            self.learning_rate = 0.01
            self.decay_rate = 0.999
            self.diffusion_rate = 0.01
            self.noise_scale = 0.001
            self.prediction_weight = 0.1
            self.emergent_dynamics = False
            self.meta_learning_rate = 0.0
            self.cross_scale_strength = 0.0
        
        # Tracking
        self.cycle_count = 0
        self.brain_cycles = 0  # Compatibility alias for monitoring
        self.experience_count = 0  # For telemetry compatibility
        self.last_prediction_error = 0.0
        self.emergence_metrics = {
            'cross_scale_coherence': 0.0,
            'meta_adaptation_rate': 0.0,
            'hierarchical_depth': len(self.levels),
            'total_parameters': self.scale_config.total_params
        }
        
        # Practical metrics for monitoring
        self._practical_metrics = {
            'field_energy': 0.0,
            'field_mean': 0.0,
            'field_std': 0.0,
            'prediction_error': 0.0,
            'motor_strength': 0.0,
            'sensory_resonance': 0.0,
            'learning_events': 0
        }
        
        # Compatibility attributes for legacy code and telemetry
        self.field = self.levels[0].field  # Primary field reference
        self.unified_field = self.field  # Alias for compatibility
        self.tensor_shape = tuple(list(self.field.shape))  # For telemetry
        self.total_dimensions = sum(self.tensor_shape)
        
        # Additional telemetry compatibility
        self.working_memory = []
        self.memory_regions = []
        self.experiences = []
        self._current_prediction_confidence = 0.5
        self._last_prediction_error = 0.0
        self._prediction_confidence_history = []
        self.enhanced_dynamics = None  # We use emergent_dynamics instead
        self.blended_reality = None  # Not used in PureFieldBrain
        
        # Log configuration
        import logging
        self.logger = logging.getLogger(__name__)
        self.logger.info(f"ðŸ§  PureFieldBrain initialized: {scale_config.name} scale, "
                        f"{scale_config.total_params:,} parameters, "
                        f"{len(self.levels)} hierarchical levels")
        
    def forward(self, sensory_input: torch.Tensor, reward: float = 0.0) -> torch.Tensor:
        """
        Hierarchical forward pass with cross-scale information flow.
        Complexity emerges from multi-resolution dynamics.
        """
        self.cycle_count += 1
        self.brain_cycles += 1  # Keep compatibility alias in sync
        self.experience_count += 1
        
        # Ensure input is on device and correct shape
        if not torch.is_tensor(sensory_input):
            sensory_input = torch.tensor(sensory_input, device=self.device, dtype=torch.float32)
        else:
            sensory_input = sensory_input.to(self.device).float()
            
        if sensory_input.dim() == 0:
            sensory_input = sensory_input.unsqueeze(0)
            
        # SAFETY: Sanitize input - remove NaN/Inf and clamp to reasonable range
        if torch.isnan(sensory_input).any() or torch.isinf(sensory_input).any():
            self.logger.warning("NaN/Inf detected in sensory input, replacing with zeros")
            sensory_input = torch.nan_to_num(sensory_input, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # Clamp inputs to reasonable range
        sensory_input = torch.clamp(sensory_input, min=-10.0, max=10.0)
        
        # Handle variable input dimensions dynamically
        actual_input_dim = sensory_input.shape[0]
        if actual_input_dim != self.input_dim:
            # Recreate input projection with correct dimensions if needed
            first_channels = self.scale_config.levels[0][1]
            if not hasattr(self, '_dynamic_input_projection') or self._last_input_dim != actual_input_dim:
                self._dynamic_input_projection = nn.Linear(actual_input_dim, first_channels, device=self.device, bias=False)
                self._last_input_dim = actual_input_dim
        
        # Everything in one fused operation
        with torch.amp.autocast(device_type=self.device, enabled=(self.device == 'cuda')):
            
            # 1. Inject sensory into first level
            sensory_field = self._inject_sensory_hierarchical(sensory_input)
            
            # 2. Hierarchical evolution with cross-scale flow
            self._evolve_hierarchical(reward)
            
            # 3. Add sensory influence to first level
            self.levels[0].field = self.levels[0].field + sensory_field * self.learning_rate
            
            # 4. Apply nonlinearity at each level
            for level in self.levels:
                level.field = torch.tanh(level.field)
            
            # Update compatibility references
            self.field = self.levels[0].field  # Keep primary field updated
            self.unified_field = self.field
            
            # 5. Extract motor from appropriate level
            motor_output = self._extract_motor_hierarchical()
            
            # 6. Update emergence metrics
            self._update_emergence_metrics()
            
            # 7. Update practical metrics (every 10 cycles for performance)
            if self.cycle_count % 10 == 0:
                self._update_practical_metrics(sensory_input, motor_output)
            
        return motor_output
    
    def _evolve_hierarchical(self, reward: float = 0.0):
        """
        Hierarchical field evolution with cross-scale information flow.
        Each level evolves with influence from adjacent scales.
        """
        # Collect evolved states
        evolved_levels = []
        
        # Bottom-up pass: evolve each level
        for i, level in enumerate(self.levels):
            # Get meta-modulation for this level
            meta_mod = None
            if self.emergent_dynamics and level.meta_channels > 0:
                meta_mod = level.field[:, :, :, -level.meta_channels:]
            
            # Evolve this level
            evolved = level.evolve(
                self.diffusion_rate,
                self.decay_rate,
                self.noise_scale,
                meta_mod
            )
            evolved_levels.append(evolved)
        
        # Cross-scale information flow (if multi-level)
        if len(self.levels) > 1 and self.cross_scale_strength > 0:
            # Top-down pass: coarse influences fine
            for i in range(len(self.levels) - 1, 0, -1):
                coarse_level = evolved_levels[i]
                fine_level = evolved_levels[i-1]
                
                # Upsample coarse to influence fine
                coarse_size = self.scale_config.levels[i][0]
                fine_size = self.scale_config.levels[i-1][0]
                
                if coarse_size < fine_size:
                    # Upsample coarse field
                    coarse_reshaped = coarse_level.permute(3, 0, 1, 2).unsqueeze(0)
                    upsampled = F.interpolate(
                        coarse_reshaped,
                        size=(fine_size, fine_size, fine_size),
                        mode='trilinear',
                        align_corners=False
                    )
                    upsampled = upsampled.squeeze(0).permute(1, 2, 3, 0)
                    
                    # Apply cross-scale connection
                    if i-1 < len(self.cross_scale_connections):
                        connection = self.cross_scale_connections[i-1]
                        # Project channels
                        coarse_channels = upsampled.shape[-1]
                        fine_channels = fine_level.shape[-1]
                        if coarse_channels != fine_channels:
                            upsampled_proj = upsampled.view(-1, coarse_channels) @ connection.T
                            upsampled = upsampled_proj.view(fine_size, fine_size, fine_size, fine_channels)
                    
                    # Blend with fine level
                    evolved_levels[i-1] = fine_level * (1 - self.cross_scale_strength) + \
                                         upsampled * self.cross_scale_strength
            
            # Bottom-up pass: fine influences coarse (pooling)
            for i in range(len(self.levels) - 1):
                fine_level = evolved_levels[i]
                coarse_level = evolved_levels[i+1]
                
                fine_size = self.scale_config.levels[i][0]
                coarse_size = self.scale_config.levels[i+1][0]
                
                if fine_size > coarse_size:
                    # Downsample fine field
                    fine_reshaped = fine_level.permute(3, 0, 1, 2).unsqueeze(0)
                    downsampled = F.adaptive_avg_pool3d(
                        fine_reshaped,
                        (coarse_size, coarse_size, coarse_size)
                    )
                    downsampled = downsampled.squeeze(0).permute(1, 2, 3, 0)
                    
                    # Project channels if needed
                    fine_channels = downsampled.shape[-1]
                    coarse_channels = coarse_level.shape[-1]
                    if fine_channels != coarse_channels and i < len(self.cross_scale_connections):
                        connection = self.cross_scale_connections[i]
                        downsampled_proj = downsampled.view(-1, fine_channels) @ connection
                        downsampled = downsampled_proj.view(coarse_size, coarse_size, coarse_size, coarse_channels)
                    
                    # Blend with coarse level
                    evolved_levels[i+1] = coarse_level * (1 - self.cross_scale_strength * 0.5) + \
                                         downsampled * (self.cross_scale_strength * 0.5)
        
        # Update fields with evolved states
        for i, (level, evolved) in enumerate(zip(self.levels, evolved_levels)):
            level.field = evolved
            
            # Add reward modulation to meta-channels
            if reward != 0 and level.meta_channels > 0:
                level.field[:, :, :, -level.meta_channels:] += reward * 0.1
    
    def _inject_sensory_hierarchical(self, sensory_input: torch.Tensor) -> torch.Tensor:
        """
        Inject sensory input into the first hierarchical level.
        Let resonance patterns emerge naturally.
        """
        # Project input to first level's channels using the appropriate projection
        first_level = self.levels[0]
        if hasattr(self, '_dynamic_input_projection') and self._last_input_dim != self.input_dim:
            sensory_channels = self._dynamic_input_projection(sensory_input)
        else:
            sensory_channels = self.input_projection(sensory_input)
        
        # Create spatial pattern through resonance
        field_perturbation = torch.zeros_like(first_level.field)
        
        # Find most responsive region (emergent specialization)
        with torch.no_grad():
            # Compute resonance with current field state
            field_flat = first_level.field.view(-1, first_level.channels)
            resonance = F.cosine_similarity(
                field_flat,
                sensory_channels.unsqueeze(0),
                dim=1
            )
            
            # Inject at peak resonance location (soft selection)
            resonance_soft = F.softmax(resonance * 5.0, dim=0)
            injection_weights = resonance_soft.view(
                first_level.field_size, first_level.field_size, first_level.field_size, 1
            )
        
        # Apply injection with spatial spread
        field_perturbation = injection_weights * sensory_channels.view(1, 1, 1, -1)
        
        return field_perturbation
    
    def _extract_motor_hierarchical(self) -> torch.Tensor:
        """
        Extract motor commands from hierarchical field gradients.
        Different levels contribute different aspects of behavior.
        """
        motor_contributions = []
        
        # Extract from each level with different weights
        for i, level in enumerate(self.levels):
            field = level.field
            
            # Compute field gradients at this level
            # Use the appropriate number of channels for this level
            num_channels = min(level.channels, level.channels - level.meta_channels if self.emergent_dynamics else level.channels)
            
            if field.shape[0] > 1:
                grad_x = field[1:, :, :, :num_channels] - field[:-1, :, :, :num_channels]
                grad_y = field[:, 1:, :, :num_channels] - field[:, :-1, :, :num_channels]
                grad_z = field[:, :, 1:, :num_channels] - field[:, :, :-1, :num_channels]
                
                # Pool gradients
                grad_magnitude = (
                    grad_x.abs().mean(dim=(0, 1, 2)) +
                    grad_y.abs().mean(dim=(0, 1, 2)) +
                    grad_z.abs().mean(dim=(0, 1, 2))
                ) / 3.0
            else:
                # For very small fields, use the field directly
                grad_magnitude = field[:, :, :, :num_channels].abs().mean(dim=(0, 1, 2))
            
            # Weight by hierarchical level (coarse = strategic, fine = tactical)
            level_weight = 1.0 / (i + 1) if i < len(self.levels) - 1 else 2.0
            motor_contributions.append(grad_magnitude * level_weight)
        
        # Combine contributions
        if len(motor_contributions) > 1:
            # Weighted combination across levels
            combined = sum(motor_contributions) / len(motor_contributions)
        else:
            combined = motor_contributions[0]
        
        # Project to motor space using last level's projection
        # (or first level if single-level)
        if len(self.levels) > 1:
            # Need to project from last level's channels to output
            last_channels = self.scale_config.levels[-1][1]
            if combined.shape[0] > last_channels:
                combined = combined[:last_channels]
            elif combined.shape[0] < last_channels:
                combined = F.pad(combined, (0, last_channels - combined.shape[0]))
        
        # Fix: output_projection is [channels_in, output_dim], so we need the first dimension
        channels_available = combined.shape[0]
        channels_needed = self.output_projection.shape[0]
        
        # Ensure combined has the right number of channels
        if channels_available > channels_needed:
            combined = combined[:channels_needed]
        elif channels_available < channels_needed:
            combined = F.pad(combined, (0, channels_needed - channels_available))
        
        # Now project to motor space: [channels] @ [channels, output_dim] = [output_dim]
        motor_raw = combined @ self.output_projection
        
        # Dynamic threshold based on meta-state
        if self.emergent_dynamics and self.levels[0].meta_channels > 0:
            meta_energy = self.levels[0].field[:, :, :, -self.levels[0].meta_channels:].mean()
            threshold = 0.1 * (2.0 - meta_energy)
        else:
            threshold = 0.1
        
        # Apply threshold
        motor_output = torch.where(
            motor_raw.abs() > threshold,
            motor_raw * 0.5,
            torch.zeros_like(motor_raw)
        )
        
        # SAFETY: Clamp motor outputs to safe range
        motor_output = torch.clamp(motor_output[:self.output_dim], min=-1.0, max=1.0)
        
        return motor_output
    
    def _update_emergence_metrics(self):
        """
        Track emergent properties that appear at scale.
        """
        if len(self.levels) > 1:
            # Measure cross-scale coherence
            coherence = 0.0
            for i in range(len(self.levels) - 1):
                level1 = self.levels[i].field
                level2 = self.levels[i+1].field
                
                # Downsample level1 to match level2 size
                size1 = level1.shape[0]
                size2 = level2.shape[0]
                if size1 > size2:
                    level1_reshaped = level1.permute(3, 0, 1, 2).unsqueeze(0)
                    level1_down = F.adaptive_avg_pool3d(level1_reshaped, (size2, size2, size2))
                    level1_down = level1_down.squeeze(0).permute(1, 2, 3, 0)
                    
                    # Compute correlation
                    corr = F.cosine_similarity(
                        level1_down.flatten(),
                        level2.flatten(),
                        dim=0
                    )
                    coherence += corr.item()
            
            self.emergence_metrics['cross_scale_coherence'] = coherence / (len(self.levels) - 1)
        
        # Track meta-adaptation rate
        if self.emergent_dynamics:
            meta_activity = 0.0
            for level in self.levels:
                if level.meta_channels > 0:
                    meta_activity += level.field[:, :, :, -level.meta_channels:].abs().mean().item()
            self.emergence_metrics['meta_adaptation_rate'] = meta_activity / len(self.levels)
    
    def _update_practical_metrics(self, sensory_input: torch.Tensor, motor_output: torch.Tensor):
        """
        Update practical metrics for monitoring learning progress.
        """
        with torch.no_grad():
            # Field energy (average activation across all levels)
            total_energy = 0.0
            for level in self.levels:
                total_energy += level.field.abs().mean().item()
            self._practical_metrics['field_energy'] = total_energy / len(self.levels)
            
            # Field statistics from first level
            first_field = self.levels[0].field
            self._practical_metrics['field_mean'] = first_field.mean().item()
            self._practical_metrics['field_std'] = first_field.std().item()
            
            # Motor strength
            self._practical_metrics['motor_strength'] = motor_output.abs().mean().item()
            
            # Sensory resonance (how well field responds to input)
            field_flat = first_field.view(-1, first_field.shape[-1])
            # Fix: input_projection is nn.Linear, use it properly
            sensory_proj = self.input_projection(sensory_input)
            resonance = F.cosine_similarity(
                field_flat.mean(dim=0),
                sensory_proj,
                dim=0
            )
            self._practical_metrics['sensory_resonance'] = float(resonance.item() if torch.is_tensor(resonance) else resonance)
            
            # Log significant events
            if self.cycle_count % 100 == 0:
                self.logger.debug(f"Cycle {self.cycle_count}: "
                                f"Energy={self._practical_metrics['field_energy']:.3f}, "
                                f"Motor={self._practical_metrics['motor_strength']:.3f}, "
                                f"Resonance={self._practical_metrics['sensory_resonance']:.3f}")
            
            # Log learning milestones
            if self._practical_metrics['field_energy'] > 0.5 and self.cycle_count > 100:
                if self.cycle_count % 1000 == 0:
                    self.logger.info(f"ðŸŽ¯ Learning milestone at cycle {self.cycle_count}: "
                                   f"Field developing structure (energy={self._practical_metrics['field_energy']:.3f})")
    
    def learn_from_prediction_error(self, actual: torch.Tensor, predicted: torch.Tensor):
        """
        Hierarchical learning from prediction error.
        Meta-learning channels modulate adaptation rates.
        """
        if not torch.is_tensor(actual):
            actual = torch.tensor(actual, device=self.device, dtype=torch.float32)
        if not torch.is_tensor(predicted):
            predicted = torch.tensor(predicted, device=self.device, dtype=torch.float32)
            
        error = actual - predicted
        self.last_prediction_error = error.abs().mean().item()
        
        # Update each level based on error
        if self.training and self.last_prediction_error > 0.01:
            for i, level in enumerate(self.levels):
                # Compute error field for this level using appropriate projection
                if hasattr(self, '_dynamic_input_projection') and hasattr(self, '_last_input_dim'):
                    # Ensure error has correct dimensions for projection
                    if error.shape[0] == self._last_input_dim:
                        error_proj = self._dynamic_input_projection(error)
                    else:
                        error_proj = self.input_projection(error)
                else:
                    error_proj = self.input_projection(error)
                
                error_field = error_proj.view(1, 1, 1, -1)
                
                # Scale learning by level (finer levels learn faster)
                level_scale = 1.0 / (i + 1)
                
                # Update evolution kernel
                kernel_update = torch.randn_like(level.evolution_kernel) * error_field.mean()
                level.evolution_kernel.data += kernel_update * self.learning_rate * level_scale * 0.1
                
                # Update meta-learning if enabled
                if self.emergent_dynamics:
                    level.update_meta_learning(self.last_prediction_error * level_scale)
            
            # Update cross-scale connections
            if len(self.cross_scale_connections) > 0 and self.cross_scale_strength > 0:
                for connection in self.cross_scale_connections:
                    connection.data += torch.randn_like(connection) * self.last_prediction_error * 0.001
    
    def get_state_dict(self) -> Dict[str, Any]:
        """Hierarchical state for persistence"""
        state = {
            'scale_config_name': self.scale_config.name,
            'cycle_count': self.cycle_count,
            'levels': [],
            'cross_scale_connections': [],
            'input_projection': self.input_projection.weight.detach().cpu().numpy() if hasattr(self.input_projection, 'weight') else self.input_projection.detach().cpu().numpy(),
            'output_projection': self.output_projection.detach().cpu().numpy(),
            'input_dim': self.input_dim,
            'output_dim': self.output_dim,
            'params': {
                'learning_rate': self.learning_rate,
                'decay_rate': self.decay_rate,
                'diffusion_rate': self.diffusion_rate,
                'noise_scale': self.noise_scale,
                'emergent_dynamics': self.emergent_dynamics,
                'meta_learning_rate': self.meta_learning_rate,
                'cross_scale_strength': self.cross_scale_strength
            },
            'emergence_metrics': self.emergence_metrics
        }
        
        # Save each level
        for level in self.levels:
            level_state = {
                'field': level.field.detach().cpu().numpy(),
                'evolution_kernel': level.evolution_kernel.detach().cpu().numpy(),
            }
            if level.meta_kernel is not None:
                level_state['meta_kernel'] = level.meta_kernel.detach().cpu().numpy()
            state['levels'].append(level_state)
        
        # Save cross-scale connections
        for connection in self.cross_scale_connections:
            state['cross_scale_connections'].append(connection.detach().cpu().numpy())
        
        return state
    
    def load_state_dict(self, state: Dict[str, Any]):
        """Load hierarchical state"""
        self.cycle_count = state.get('cycle_count', 0)
        
        # Handle different input projection formats
        if hasattr(self.input_projection, 'weight'):
            self.input_projection.weight.data = torch.tensor(state['input_projection'], device=self.device)
        else:
            self.input_projection.data = torch.tensor(state['input_projection'], device=self.device)
            
        self.output_projection.data = torch.tensor(state['output_projection'], device=self.device)
        
        # Update input/output dimensions if changed
        if 'input_dim' in state and state['input_dim'] != self.input_dim:
            self.input_dim = state['input_dim']
        if 'output_dim' in state and state['output_dim'] != self.output_dim:
            self.output_dim = state['output_dim']
        
        # Load each level
        for i, level_state in enumerate(state.get('levels', [])):
            if i < len(self.levels):
                self.levels[i].field = torch.tensor(level_state['field'], device=self.device)
                self.levels[i].evolution_kernel.data = torch.tensor(level_state['evolution_kernel'], device=self.device)
                if 'meta_kernel' in level_state and self.levels[i].meta_kernel is not None:
                    self.levels[i].meta_kernel.data = torch.tensor(level_state['meta_kernel'], device=self.device)
        
        # Load cross-scale connections
        for i, connection_state in enumerate(state.get('cross_scale_connections', [])):
            if i < len(self.cross_scale_connections):
                self.cross_scale_connections[i].data = torch.tensor(connection_state, device=self.device)
        
        # Load params if present
        if 'params' in state:
            for key, value in state['params'].items():
                setattr(self, key, value)
        
        # Load emergence metrics
        if 'emergence_metrics' in state:
            self.emergence_metrics.update(state['emergence_metrics'])
        
        # Update compatibility attributes after loading
        if len(self.levels) > 0:
            self.field = self.levels[0].field
            self.unified_field = self.field
            self.tensor_shape = tuple(list(self.field.shape))
    
    @property
    def metrics(self) -> Dict[str, float]:
        """Hierarchical metrics for monitoring"""
        # Combine practical metrics with computed metrics
        metrics = dict(self._practical_metrics)  # Start with practical metrics
        metrics.update({
            'prediction_error': self.last_prediction_error,
            'cycle_count': self.cycle_count,
            'hierarchical_depth': len(self.levels),
            'total_parameters': self.scale_config.total_params,
        })
        
        # Add emergence metrics
        metrics.update(self.emergence_metrics)
        
        return metrics
    
    def reset(self):
        """Reset hierarchical state"""
        self.cycle_count = 0
        self.last_prediction_error = 0.0
        
        # Reset each level
        for level in self.levels:
            level.field.zero_()
            level.evolution_kernel.data = torch.randn_like(level.evolution_kernel) * 0.02
            if level.meta_kernel is not None:
                level.meta_kernel.data = torch.randn_like(level.meta_kernel) * 0.01
        
        # Reset cross-scale connections
        for connection in self.cross_scale_connections:
            connection.data = torch.randn_like(connection) * 0.1
        
        # Reset emergence metrics
        self.emergence_metrics = {
            'cross_scale_coherence': 0.0,
            'meta_adaptation_rate': 0.0,
            'hierarchical_depth': len(self.levels),
            'total_parameters': self.scale_config.total_params
        }
        
        # Update compatibility attributes after reset
        if len(self.levels) > 0:
            self.field = self.levels[0].field
            self.unified_field = self.field
            self.tensor_shape = tuple(list(self.field.shape))
        
    def get_brain_state(self) -> Dict[str, Any]:
        """Get brain state for telemetry (compatibility method)"""
        return {
            'brain_cycles': self.brain_cycles,
            'cycle_count': self.cycle_count,
            'field_energy': float(torch.norm(self.field).item()) if torch.is_tensor(self.field) else 0.0,
            'field_information': float(self.field.abs().mean().item()) if torch.is_tensor(self.field) else 0.0,
            'max_activation': float(self.field.abs().max().item()) if torch.is_tensor(self.field) else 0.0,
            'stability_index': 0.9,  # PureFieldBrain is inherently stable
            'cognitive_mode': 'emergent',
            'active_constraints': 0,
            'cycle_time_ms': 0.0,
            'prediction_confidence': self._current_prediction_confidence,
            'prediction_error': self._last_prediction_error
        }
    
    def _create_brain_state(self) -> Dict[str, Any]:
        """Alternative method name for compatibility"""
        return self.get_brain_state()
    
    def process(self, sensory_input: torch.Tensor, reward: float = 0.0) -> torch.Tensor:
        """Compatibility alias for forward()"""
        return self.forward(sensory_input, reward)
    
    def __repr__(self) -> str:
        levels_desc = ", ".join([f"{size}Â³Ã—{ch}" for size, ch in self.scale_config.levels])
        return (f"PureFieldBrain("
                f"scale={self.scale_config.name}, "
                f"levels=[{levels_desc}], "
                f"params={self.scale_config.total_params:,}, "
                f"emergent={self.emergent_dynamics}, "
                f"device={self.device})")


# Convenience functions
def create_pure_field_brain(
    input_dim: int = 10,
    output_dim: int = 4,
    size: str = 'medium',
    aggressive: bool = True,
    device: str = None
) -> PureFieldBrain:
    """
    Create a hierarchically scalable PureFieldBrain.
    
    Sizes with emergent properties:
    - 'tiny': Single 16Â³Ã—32 (131k params) - basic reflexes
    - 'small': Single 24Â³Ã—48 (663k params) - simple behaviors
    - 'medium': Single 32Â³Ã—64 (2.1M params) - complex behaviors
    - 'large': Dual [32Â³Ã—96, 16Â³Ã—48] (10M params) - strategic planning emerges
    - 'huge': Triple [48Â³Ã—128, 24Â³Ã—64, 12Â³Ã—32] (50M params) - abstract reasoning emerges
    - 'massive': Quad [64Â³Ã—256, 32Â³Ã—128, 16Â³Ã—64, 8Â³Ã—32] (100M+ params) - full emergence
    
    Emergent properties by scale:
    - <2M params: Basic sensorimotor coordination
    - 2-10M params: Pattern recognition and simple planning
    - 10-50M params: Hierarchical reasoning and meta-learning
    - 50-100M params: Abstract concepts and self-modification
    - >100M params: True autonomous intelligence
    """
    if size not in SCALE_CONFIGS:
        raise ValueError(f"Size must be one of {list(SCALE_CONFIGS.keys())}")
    
    scale_config = SCALE_CONFIGS[size]
    
    return PureFieldBrain(
        input_dim=input_dim,
        output_dim=output_dim,
        scale_config=scale_config,
        device=device,
        aggressive=aggressive
    )


if __name__ == "__main__":
    import time
    
    print("=" * 60)
    print("PureFieldBrain Hierarchical Scaling Demonstration")
    print("=" * 60)
    
    # Test different scales
    for size_name in ['tiny', 'medium', 'large', 'huge']:
        print(f"\n{'='*20} {size_name.upper()} SCALE {'='*20}")
        
        brain = create_pure_field_brain(size=size_name, aggressive=True)
        print(brain)
        print(f"Actual parameters: {sum(p.numel() for p in brain.parameters()):,}")
        
        # Test forward pass
        sensory = torch.randn(10)
        motor = brain(sensory)
        
        # Show initial metrics
        metrics = brain.metrics
        print(f"\nKey metrics:")
        print(f"  - Hierarchical depth: {metrics['hierarchical_depth']}")
        print(f"  - Total parameters: {metrics['total_parameters']:,}")
        print(f"  - Cross-scale coherence: {metrics['cross_scale_coherence']:.3f}")
        print(f"  - Meta-adaptation rate: {metrics['meta_adaptation_rate']:.3f}")
        
        # Test emergent properties
        if brain.emergent_dynamics:
            print(f"\nâœ“ EMERGENT DYNAMICS ENABLED at {brain.scale_config.total_params:,} params")
            print(f"  - Meta-learning rate: {brain.meta_learning_rate}")
            print(f"  - Cross-scale strength: {brain.cross_scale_strength}")
        else:
            print(f"\nâœ— Pre-emergent stage ({brain.scale_config.total_params:,} params)")
        
        # Quick performance test
        if torch.cuda.is_available() and size_name != 'huge':  # Skip huge for quick test
            brain = brain.cuda()
            
            # Warmup
            for _ in range(10):
                brain(torch.randn(10, device='cuda'))
            
            # Benchmark
            torch.cuda.synchronize()
            start = time.perf_counter()
            cycles = 100
            for _ in range(cycles):
                brain(torch.randn(10, device='cuda'))
            torch.cuda.synchronize()
            elapsed = time.perf_counter() - start
            
            ms_per_cycle = (elapsed / cycles) * 1000
            print(f"\nPerformance: {ms_per_cycle:.2f}ms/cycle ({1000/ms_per_cycle:.1f} Hz)")
    
    # Demonstrate cross-scale information flow
    print(f"\n{'='*60}")
    print("CROSS-SCALE INFORMATION FLOW DEMONSTRATION")
    print("=" * 60)
    
    # Create a large brain with multiple levels
    brain = create_pure_field_brain(size='large', aggressive=True)
    print(f"\nTesting {brain.scale_config.name} configuration:")
    print(f"Levels: {brain.scale_config.levels}")
    
    # Run a few cycles to establish patterns
    for i in range(10):
        sensory = torch.randn(10)
        motor = brain(sensory, reward=0.1 if i % 3 == 0 else 0.0)
    
    # Check cross-scale coherence
    metrics = brain.metrics
    print(f"\nAfter 10 cycles:")
    print(f"  - Cross-scale coherence: {metrics['cross_scale_coherence']:.3f}")
    print(f"  - Level 0 energy: {metrics['level_0_energy']:.2f}")
    if 'level_1_energy' in metrics:
        print(f"  - Level 1 energy: {metrics['level_1_energy']:.2f}")
    
    # Test learning
    print(f"\nLearning from prediction error:")
    for i in range(5):
        actual = torch.randn(10)
        predicted = torch.randn(10)
        brain.learn_from_prediction_error(actual, predicted)
        print(f"  Cycle {i+1}: error = {brain.last_prediction_error:.3f}")
    
    print(f"\n{'='*60}")
    print("Hierarchical scaling enables true intelligence emergence!")
    print("=" * 60)